{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2-345M",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadeevla/gpt2/blob/master/GPT2_345M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5G-mEdprsHs",
        "colab_type": "code",
        "outputId": "5c16da5a-6424-49a4-b691-9e8f9236ff06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 174, done.\u001b[K\n",
            "remote: Total 174 (delta 0), reused 0 (delta 0), pack-reused 174\u001b[K\n",
            "Receiving objects: 100% (174/174), 4.35 MiB | 15.59 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9uu3id04XyK",
        "colab_type": "code",
        "outputId": "5b3dcd24-67c8-4c84-e8c4-29cd4e6d1317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT3nE9Vjr2Kt",
        "colab_type": "code",
        "outputId": "10f241b0-4bc1-4556-85c7-ec7024d6cb66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install textwrap3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 26.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.0.18 has requirement regex==2018.01.10, but you'll have regex 2017.4.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, tqdm\n",
            "  Found existing installation: regex 2018.1.10\n",
            "    Uninstalling regex-2018.1.10:\n",
            "      Successfully uninstalled regex-2018.1.10\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5 tqdm-4.31.1\n",
            "Collecting textwrap3\n",
            "  Downloading https://files.pythonhosted.org/packages/77/9c/a53e561d496ee5866bbeea4d3a850b3b545ed854f8a21007c1e0d872e94d/textwrap3-0.9.2-py2.py3-none-any.whl\n",
            "Installing collected packages: textwrap3\n",
            "Successfully installed textwrap3-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo7bS1iGsXvG",
        "colab_type": "code",
        "outputId": "47e8a518-5147-4e83-de4c-8df8d7ca4907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!python download_model.py 345M"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 1.07Mit/s]                                                     \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 57.6Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.11Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:20, 69.8Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 9.25Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 46.9Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 53.6Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sn9hrva4szX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#update code to use 345M model\n",
        "!sed -i -e 's/117M/345M/g' ./src/generate_unconditional_samples.py\n",
        "!sed -i -e 's/117M/345M/g' ./src/interactive_conditional_samples.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjSbhybc7euF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#avoid long strings in colab, wrap output (any other ideas to wrap output?)\n",
        "!sed -i -e \"s/import tensorflow as tf/import tensorflow as tf\\nfrom textwrap import wrap /g\" ./src/generate_unconditional_samples.py\n",
        "!sed -i -e \"s/print(text)/print(*wrap(text, width=80), sep=\\'\\\\\\n\\')/g\" ./src/generate_unconditional_samples.py\n",
        "\n",
        "!sed -i -e \"s/import tensorflow as tf/import tensorflow as tf\\nfrom textwrap import wrap /g\" ./src/interactive_conditional_samples.py\n",
        "!sed -i -e \"s/print(text)/print(*wrap(text, width=80), sep=\\'\\\\\\n\\')/g\" ./src/interactive_conditional_samples.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl1f_s7i9bAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "98958171-2a96-49d8-bd47-a1b57b334e6d"
      },
      "source": [
        "!python ./src/generate_unconditional_samples.py --top_k=40 --seed=1 --temperature=0.9 --length=40 --nsamples=5"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-10 09:41:36.963627: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-10 09:41:36.963971: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x270b5a0 executing computations on platform Host. Devices:\n",
            "2019-05-10 09:41:36.964003: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-10 09:41:37.186883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-10 09:41:37.187416: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x270b2e0 executing computations on platform CUDA. Devices:\n",
            "2019-05-10 09:41:37.187446: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-10 09:41:37.187788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-10 09:41:37.187814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-10 09:41:38.471910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-10 09:41:38.471972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-10 09:41:38.471984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-10 09:41:38.472239: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-10 09:41:38.472311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-05-10 09:41:47.917541: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            "Hear from some of the best professional hockey broadcasters in the business!\n",
            "Listen to all of our interviews from 2013 and check out this year's highlights\n",
            "1. Jeff Marek: The\n",
            "======================================== SAMPLE 2 ========================================\n",
            "The UESPWiki – Your source for The Elder Scrolls since 1995  Kiln of Skyrim\n",
            "Kiln of Skyrim is the primary dungeon of the stronghold of the Imperial Legion\n",
            "in\n",
            "======================================== SAMPLE 3 ========================================\n",
            "Here you can find all the resources for you to follow your child's physical\n",
            "development and mental development. For the most up-to-date information, please\n",
            "do not hesitate to ask your school or organization\n",
            "======================================== SAMPLE 4 ========================================\n",
            "Photo Gallery  A collection of art that illustrates a particular theme in music,\n",
            "visual or text.  Click to expand...<|endoftext|>(CNN) A former U.S. Navy SEAL\n",
            "testified that\n",
            "======================================== SAMPLE 5 ========================================\n",
            "I'm glad to hear that the folks at DBSD are listening. I've been following the\n",
            "progress of the project for awhile now, but I have to admit, I'm not totally\n",
            "convinced it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9_EcItb47yi",
        "colab_type": "code",
        "outputId": "33f129cf-82f9-4af4-cbb5-3c521ad51d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1312
        }
      },
      "source": [
        "!python ./src/interactive_conditional_samples.py --top_k=40 --seed=1 --temperature=0.9 --length=40 --nsamples=5"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-10 11:14:18.393001: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-10 11:14:18.393250: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1740520 executing computations on platform Host. Devices:\n",
            "2019-05-10 11:14:18.393290: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-10 11:14:18.548505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-10 11:14:18.548989: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x173fe40 executing computations on platform CUDA. Devices:\n",
            "2019-05-10 11:14:18.549027: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-10 11:14:18.549444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-10 11:14:18.549468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-10 11:14:18.988188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-10 11:14:18.988243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-10 11:14:18.988256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-10 11:14:18.988493: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-10 11:14:18.988533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> What's outside the simulation?\n",
            "2019-05-10 11:14:33.541253: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            " What is inside?  This is what I would like to explore now, but I am afraid this\n",
            "isn't done yet.  In order to see this effect fully I need to get some\n",
            "======================================== SAMPLE 2 ========================================\n",
            " What is beyond the simulation? What's inside? Are there monsters outside the\n",
            "simulation? Are there other human beings outside the simulation?  The story and\n",
            "the simulation are intertwined. There are many things\n",
            "======================================== SAMPLE 3 ========================================\n",
            " That can just be what's within the simulation itself; this is how we think.\"\n",
            "If you can't believe it, let me tell you. \"The world really does exist.\" That's\n",
            "======================================== SAMPLE 4 ========================================\n",
            " Not much for the most part. The players are not in a real world situation or\n",
            "environment like they are in a game. But in the case of FIFA 14, the players'\n",
            "bodies are moving inside\n",
            "======================================== SAMPLE 5 ========================================\n",
            "  There's always a few variables that could break things. The best advice I can\n",
            "give you is to go back to the drawing board!  What are you excited about?  I\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
            "    yield g\n",
            "  File \"./src/interactive_conditional_samples.py\", line 69, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"./src/interactive_conditional_samples.py\", line 87, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"./src/interactive_conditional_samples.py\", line 84, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
            "    tf_session.TF_CloseSession(self._session)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyiGymxuz7MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}